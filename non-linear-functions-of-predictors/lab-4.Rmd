## STATS 763: Lab 4: Nonlinear functions of predictors
#### Tarin Eccleston
#### 10/08/2024

```{r results='hide', warning=FALSE, message=FALSE, error=FALSE, echo=FALSE}
library(tidyverse)
library(stringr)
library(lubridate)
library(splines)
```

### Quadratics
#### Q1) Simulate x <- rexp(100) and y <- 4*exp(-x)+rnorm(100). Plot y against x and overlay the true form of ð¸[ð‘Œ|ð‘‹=ð‘¥] and the fitted value from a linear model with x and x^2 as predictors. How would you describe the regression results to avoid giving the misleading impression that the relationship changes direction?

```{r}
x <- rexp(100)
y_expected <- 4*exp(-x)
y <- 4*exp(-x)+rnorm(100)
y_x = data.frame(cbind(x, y_expected, y))

y_model_x <- lm(y ~ x, y_x)
y_model_x_squared <- lm(y ~ x + I(x^2), y_x)

y_x$y_fitted_x <- predict(y_model_x, y_x)
y_x$y_fitted_x_squared <- predict(y_model_x_squared, y_x)
```

```{r}
ggplot(y_x, aes(x = x)) +
  geom_point(aes(y = y), color = "darkgrey", alpha = 0.5) +
  geom_line(aes(y = y_expected, color = "Expected"), size = 1) +
  geom_line(aes(y = y_fitted_x, color = "Fitted x"), size = 1) +
  geom_line(aes(y = y_fitted_x_squared, color = "Fitted x^2"), size = 1) +
  labs(title = "True Curve vs Fitted Curves",
       x = "x",
       y = "y",
       color = "Legend") +
  scale_color_manual(values = c("Expected" = "black", 
                                "Fitted x" = "red", 
                                "Fitted x^2" = "blue")) +
  theme_minimal()
```

For the model fitted with x, the fitted values show that it fits the overall trend in data, but not the local exponential trend well. This is expected as the true trend is exponential and plateaus after around y = 0 at around x = 1, whereas the fit with model using x continues to grow negative.

The polynomial (model fitted with x^2) will try to fit all the data, and follows distinct behavior (going down and up). This will give over-estimates for larger values of x as the true trend is exponential. We can say that the polynomial provides sufficient support from x = 0 to x = turning point.

The turning point of a quadratic function \( y = ax^2 + bx + c \) can be found using the formula:

\[
x_{\text{turning point}} = -\frac{b}{2a}
\]

```{r}
summary(y_model_x_squared)
```

Given the coefficients for x and x^2 from the summary:

- \( a = 0.7014 \) (coefficient of \( x^2 \))
- \( b = -3.1744 \) (coefficient of \( x \))

The turning point is calculated as:

\[
x_{\text{turning point}} = -\frac{-3.1744}{2 \times 0.7014} = \frac{3.1744}{1.4028} \approx 2.263
\]

### Cubic Splines
#### Q2) Plot the data from question 1, the true functional relationship, and the fitted values for these two regression models. What is the interpretation of the coefficients in each model? Do any of the coefficients directly tell us whether there is positive slope in some region?

```{r}
y_model_x_linear_spline_1 <- lm(y ~ pmin(x,2) + pmax(pmin(x,3),2) + pmax(x,3), data = y_x)
y_model_x_linear_spline_2 <- lm(y ~ x + pmax(x,2) + pmax(x,3), data = y_x)

y_x$y_fitted_x_linear_spline_1 <- predict(y_model_x_linear_spline_1, y_x)
y_x$y_fitted_x_linear_spline_2 <- predict(y_model_x_linear_spline_2, y_x)
```

```{r}
ggplot(y_x, aes(x = x)) +
  geom_point(aes(y = y), color = "darkgrey", alpha = 0.5) +
  geom_line(aes(y = y_expected, color = "Expected"), size = 1) +
  geom_line(aes(y = y_fitted_x_linear_spline_1, color = "Fitted Linear Spline 1"), size = 1) +
  geom_line(aes(y = y_fitted_x_linear_spline_2, color = "Fitted Linear Spline 2"), size = 1) +
  labs(title = "True Curve vs Fitted Curves",
       x = "x",
       y = "y",
       color = "Legend") +
  scale_color_manual(values = c("Expected" = "black", 
                                "Fitted Linear Spline 1" = "purple", 
                                "Fitted Linear Spline 2" = "orange")) +
  theme_minimal()
```

Both fits for linear spline 1 and 2 overlap eachother. The first model (linear spline 1) creates a more complex piece-wise function of x with different segments that behave differently, while the second model (linear spline 2) uses x directly along with additional terms that modify its influence based on specific thresholds. Both fits don't capture the inherent exponential relationship in the data.

They are different ways of creating the same function, however their coefficients offer different interpretations.

```{r}
summary(y_model_x_linear_spline_1)
```

The first model (linear spline 1) segments x into different capped and floored regions, which reflects a piece-wise linear relationship with y; the slope changes at specific values of x (2 and 3). For a particular piece-wise region, the slope is determined by the coefficient value for that piece-wise region, while the intercept is determined by the intercept coefficient, plus all other coefficients for other piece-wise regions.

```{r}
summary(y_model_x_linear_spline_2)
```

The second model (linear spline 2) assumes a direct relationship between x and y. The slope is always the coefficient of x plus pmax(x,2) and/or pmax(x,3) when x crosses into their respective piece-wise regions. The intercepts for a piece-wise region are the sum of the intercept coefficient and the coefficients for all piece-wise regions that are within ranges that are currently greater than x.

### Q3)
#### In the childrenâ€™s FEV data, fit a regression of fev on smoking, gender, and age, using a cubic spline with three intervals for age. What is the estimated smoking effect?

```{r results='hide', warning=FALSE, message=FALSE, error=FALSE}
FEV <- read.table("../data/fev.txt", header = TRUE)
FEV <- FEV %>%
  mutate(sex = as.factor(sex)) %>%
  mutate(smoke = as.factor(smoke))

fev_model_cublic_splines <- lm(fev ~ smoke + sex + ns(age, 3), data = FEV)
```

```{r}
summary(fev_model_cublic_splines)
```

Assuming smoker = 2, and non-smoker = 1.

Based on the summary, smokers have on average a 0.12 liter increase in FEV compared to non-smokers when we adjust for sex, and age (using cubic splines with interval = 3).




